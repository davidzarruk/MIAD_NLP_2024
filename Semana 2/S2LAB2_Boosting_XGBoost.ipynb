{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![image info](https://raw.githubusercontent.com/albahnsen/MIAD_ML_and_NLP/main/images/banner_1.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Construcción e implementación de modelos basados en Boosting\n",
    "En este notebook aprenderá a construir e implementar modelos de Adaboost, Gradient Boosting y XGBoost. El primer modelo lo desarrollará de forma manual y usando la librería especializada sklearn, mientras que los otros dos los desarrollará solamente usando la librería."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Instrucciones Generales:\n",
    "\n",
    "Los modelos que construirá por medio de este notebook deberán predecir si un usuario deja o no de usar los servicios de una compañía (churn) teniendo en cuenta diferentes variables. Para conocer más detalles de la base de 'churn' puede ingresar al siguiente vínculo: http://srepho.github.io/Churn/Churn\n",
    "\n",
    "Para realizar la actividad, solo siga las indicaciones asociadas a cada celda del notebook. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importar base de datos y librerías\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Carga de datos de archivos .csv\n",
    "data = pd.read_csv('https://raw.githubusercontent.com/albahnsen/MIAD_ML_and_NLP/main/datasets/churn.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Selección de variables numéricas (X)\n",
    "X = data.iloc[:, [1,2,6,7,8,9,10]].astype(np.float)\n",
    "# Tranformación de variables booleanas a floats\n",
    "X = X.join((data.iloc[:, [4,5]] == 'no').astype(np.float))\n",
    "\n",
    "# Definición variable de interés binaria (y)\n",
    "y = (data.iloc[:, -1] == 'True.').astype(np.int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Separación de variables predictoras (X) y variable de interés (y) en set de entrenamiento y test usandola función train_test_split\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33, random_state=40)\n",
    "n_samples = X_train.shape[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Adaboost manual"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Definición de la cantidad de árboles de decisión del modelo\n",
    "n_estimators = 10\n",
    "\n",
    "# Definición de DataFrame para guardar pesos de las observaciones en cada árbol de decisión\n",
    "weights = pd.DataFrame(index=X_train.index, columns=list(range(n_estimators)))\n",
    "\n",
    "# Asignación los mismos pesos para todas las observaciones en el árbol 0\n",
    "t = 0\n",
    "weights[t] = 1 / n_samples\n",
    "\n",
    "# Visualización de DataFrame de pesos\n",
    "weights.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn import metrics\n",
    "\n",
    "# Definición y entrenamiento (fit) del primer árbol de decisión (DecisionTreeClassifier)\n",
    "trees = []\n",
    "trees.append(DecisionTreeClassifier(max_depth=1))\n",
    "trees[t].fit(X_train, y_train, sample_weight=weights[t].values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Estimación del error del primer árbol de decisión\n",
    "y_pred_ = trees[t].predict(X_train)\n",
    "error = []\n",
    "error.append(1 - metrics.balanced_accuracy_score(y_pred_, y_train, weights[t].values))\n",
    "error[t]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cálculo del factor alpha del primer árbol de decisión\n",
    "alpha = []\n",
    "alpha.append(np.log((1 - error[t]) / error[t])/2)\n",
    "alpha[t]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Actualización de los pesos a considerar en el segundo árbol (t+1)\n",
    "weights[t + 1] = weights[t]\n",
    "filter_ = y_pred_ != y_train\n",
    "\n",
    "weights.loc[filter_, t + 1] = weights.loc[filter_, t] * np.exp(alpha[t])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normalización de los pesos\n",
    "weights[t + 1] = weights[t + 1] / weights[t + 1].sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualización de DataFrame de pesos\n",
    "weights.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Definición de loop que itera sobre todos los árboles definidos realizando los mismos cáculos presentados anteriormente\n",
    "for t in range(1, n_estimators):\n",
    "    # Definición y entrenamiento (fit) del árbol t\n",
    "    trees.append(DecisionTreeClassifier(max_depth=1))\n",
    "    trees[t].fit(X_train, y_train, sample_weight=weights[t].values)\n",
    "    y_pred_ = trees[t].predict(X_train)\n",
    "    # Estimación del error del árbol t\n",
    "    error.append(1 - metrics.balanced_accuracy_score(y_pred_, y_train, weights[t].values))\n",
    "    # Cálculo del factor alpha para el árbol t\n",
    "    alpha.append(np.log((1 - error[t]) / error[t]) / 2)\n",
    "    # Actualización de pesos para el árbol t+2\n",
    "    weights[t + 1] = weights[t]\n",
    "    filter_ = y_pred_ != y_train\n",
    "    weights.loc[filter_, t + 1] = weights.loc[filter_, t] * np.exp(alpha[t])\n",
    "    weights[t + 1] = weights[t + 1] / weights[t + 1].sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualización de los errores de cada árbol\n",
    "error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualización de DataFrame de pesos\n",
    "weights.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Solo se usan modelos con error menor a 0.5\n",
    "new_n_estimators = np.sum([x<0.5 for x in error])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prección sobre la muestra de test\n",
    "y_pred_all = np.zeros((X_test.shape[0], new_n_estimators))\n",
    "for t in range(new_n_estimators):\n",
    "    y_pred_all[:, t] = trees[t].predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Obtención de la predicción final al poderar las predicciones por el factor aplha\n",
    "y_pred = (np.sum(y_pred_all * alpha[:new_n_estimators], axis=1) >= 1).astype(np.int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Impresión del desempeño modelo\n",
    "metrics.f1_score(y_pred, y_test.values), metrics.accuracy_score(y_pred, y_test.values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Definición de un sólo árbol de decisón para compara el desempeño\n",
    "clf = DecisionTreeClassifier()\n",
    "clf.fit(X_train, y_train)\n",
    "y_pred = clf.predict(X_test)\n",
    "metrics.f1_score(y_pred, y_test.values), metrics.accuracy_score(y_pred, y_test.values)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Adaboost usando sklearn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importación y definición de modelo AdaBoostClassifier\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "clf = AdaBoostClassifier()\n",
    "clf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Entrenamiento (fit) y desempeño del modelo AdaBoostClassifier\n",
    "clf.fit(X_train, y_train)\n",
    "y_pred = clf.predict(X_test)\n",
    "metrics.f1_score(y_pred, y_test.values), metrics.accuracy_score(y_pred, y_test.values)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gradient Boosting usando sklearn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importación y definición de modelo GradientBoostingClassifier\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "clf = GradientBoostingClassifier()\n",
    "clf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Entrenamiento (fit) y desempeño del modelo GradientBoostingClassifier\n",
    "clf.fit(X_train, y_train)\n",
    "y_pred = clf.predict(X_test)\n",
    "metrics.f1_score(y_pred, y_test.values), metrics.accuracy_score(y_pred, y_test.values)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## XGBoost usando sklearn\n",
    "Instalar la librería xgboost usando el comando: pip install xgboost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importación y definición de modelo XGBClassifier\n",
    "\n",
    "from xgboost import XGBClassifier\n",
    "clf = XGBClassifier()\n",
    "clf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Entrenamiento (fit) y desempeño del modelo XGBClassifier\n",
    "clf.fit(X_train, y_train)\n",
    "y_pred = clf.predict(X_test)\n",
    "metrics.f1_score(y_pred, y_test.values), metrics.accuracy_score(y_pred, y_test.values)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
